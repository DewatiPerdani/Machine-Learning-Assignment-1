---
title: "Machine Learning - Group Project"
author: "Group 4"
date: "1/28/2017"
output: html_document
---

```{r}
library(ggplot2)
#install.packages("class")
library(class)
```

(a) Build a k-Nearest Neighbours classifier in R for “wine_quality-white.csv” that:
1. loads the data file;
2. construct a new binary column “good wine” that indicates whether the wine is good
(which we define as having a quality of 6 or higher) or not;
3. splits the data set into a training data set (~40%), a validation data set (~30%) and a test data set (~30%) — make sure you shuffle the record before the split;
4. normalises the data according to the Z-score transform;
5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;
6. evaluates each classifier on the validation set and selects the best classifier;
7. predicts the generalisation error using the test data set, as well as outputs the result in a confusion matrix.

```{r}
#1. loads the data file;
whitew<-read.csv("winequality-white.csv",sep = ";", header = TRUE)

#2. construct a new binary column “good wine” that indicates whether the wine is good (which we define as having a quality of 6 or higher) or not
whitew$goodwine <- ifelse(whitew$quality>=6, 1, 0) #binary column for wine quality

#3. splits the data set into a training data set (~40%), a validation data set (~30%) and atest data set (~30%) — make sure you shuffle the record before the split;
set.seed(123)

# we ignore the wine quality column because it is a deterministic relationship with good_wine

#4. normalises the data according to the Z-score transform;
whitew_scale <- as.data.frame(scale(whitew[1:11]))
whitew_y <- as.data.frame(whitew[,13])
colnames(whitew_y) <- "good_wine"
whitew_scale <- cbind(whitew_scale, whitew_y)

whitew_shuffle<- whitew_scale[sample(nrow(whitew_scale)),] # shuffles observations


wtrain <- whitew_shuffle[1:1959,]
wvalid <- whitew_shuffle[1960:3429,]
wtest <- whitew_shuffle[3430:4898,] #splits data into three sets

summary(wtest)#check normalised data

#5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;

wtrain_label<-wtrain[,12]
wvalid_label<-wvalid[,12]
wtest_label<-wtest[,12]

wtrain <- wtrain[,1:11]
wvalid <- wvalid[,1:11]
wtest  <- wtest[,1:11]



white_pred <- data.frame(V1 = 1:1470)

for (i in seq(1:80)){
  white_pred[,i] <- knn(wtrain, wvalid, cl = wtrain_label, k = i)
  }

#6. evaluates each classifier on the validation set and selects the best classifier;
white_eval<-as.data.frame(matrix(0,ncol = 1, nrow = 80))
names(white_eval)[1]<-"correct.prediction"

for (i in seq(1:80)){
  white_eval[i,] <- 100*sum(wvalid_label == white_pred[,i])/1470
  }

white_eval[,"k"]<-seq(1:80)

ggplot(white_eval, aes(x = k )) + geom_line(aes(y = correct.prediction)) 
max(white_eval$correct.prediction)
#best classifier is k=18 with percentage of correct prediction 76.73469%

#7. predicts the generalisation error using the test data set, as well as outputs the result in a confusion matrix.
gen.error <- knn(wtrain, wtest, wtrain_label, k = 18)
table(gen.error, wtest_label) #confusion matrix 
100 * sum(wtest_label == gen.error)/1469 #proportion of correct classification for k=18 is 76.17427%

```