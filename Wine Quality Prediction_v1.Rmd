---
title: "Machine Learning - Group Project 1"
author: "Group 4"
date: "1/28/2017"
output: html_document
---

```{r message=FALSE}
require(ggplot2)
require(class)
require(xtable)
```

####1. Loads the data file
```{r}
set.seed(2017)
whitew<-read.csv("winequality-white.csv",sep = ";", header = TRUE)
```

####2. Construct a new binary column “good wine” that indicates whether the wine is good.
We define a good wine as having a quality of 6 or higher. We set 1 to be "good wine" and 0 to be "bad wine".
```{r}
whitew$goodwine <- ifelse(whitew$quality>=6, 1, 0) 
```

####4. Normalise the data according to the Z-score transform. 

We do not normalise the wine quality column because it is our output predictor variable and also has a deterministic relationship with good_wine. We should normalise the data before splitting it in order to ensure consistent scaling across the various split datasets.
```{r}
whitew_scale <- as.data.frame(scale(whitew[1:11]))
whitew_y <- as.data.frame(whitew[,13])
colnames(whitew_y) <- "good_wine"
whitew_scale <- cbind(whitew_scale, whitew_y)
```

####3. Split the data set into a training data set (~40%), a validation data set (~30%) and a test data set (~30%). Shuffle the record before the split. 

We also separate the y variable (good_wine) from the rest of the predictors that we use in the k-nn algorithm.
```{r}
whitew_shuffle <- whitew_scale[sample(nrow(whitew_scale)),]

wtrain <- whitew_shuffle[1:1959,]
wvalid <- whitew_shuffle[1960:3429,]
wtest <- whitew_shuffle[3430:4898,]

wtrain_label<-wtrain[,12]
wvalid_label<-wvalid[,12]
wtest_label<-wtest[,12]

wtrain <- as.data.frame(wtrain[,1:11])
wvalid <- as.data.frame(wvalid[,1:11])
wtest  <- as.data.frame(wtest[,1:11])
```

####5. Load and train the k-Nearest Neighbours classifiers for k = 1, .., 80. 

We use the best predictor from training the k Nearest Neighbors algorithm on the training set and finding the optimal value on the validation set.
```{r}
white_valid <- data.frame(V1 = 1:nrow(wvalid))

for (i in seq(1:80)) {
  white_valid[,i] <- knn(wtrain, wvalid, cl = wtrain_label, k = i)
}
```

####6. Evaluate each classifier on the validation set and select the best classifier.
```{r}
white_eval <- as.data.frame(matrix(0,ncol = 1, nrow = 80))
names(white_eval)[1]<-"correct.prediction"

for (i in seq(1:80)){
  white_eval[i,] <- 100*sum(wvalid_label == white_valid[,i])/nrow(wvalid)
  }

white_eval[,"k"]<-seq(1:80)

min.error <- max(white_eval$correct.prediction)
min.error.k <- white_eval$k[white_eval$correct.prediction == max(white_eval$correct.prediction)]
```
The best predictor is k = `r min.error.k` with a prediction accuracy on the validation set of `r round(min.error,3)`%.

####Train and validate the k-Nearest Neighbors algorithm on the training set itself. 
This code is only used for the plot afterwards and is not used to select the optimal k value.

The code below is used to create a plot to see the performance of each value of k on both the training and validation sets. As k decreases, we expect the performance of the validation set to decrease, then find a minimum, and then increase at low values of k due to overfitting. As k decreases, we expect the performance of the training set to decrease until the error is 0 when k = 1. This is because the best predictor of an observation is the observation itself.
```{r}
white_train <- data.frame(V1 = 1:nrow(wtrain))

for (i in seq(1:80)) {
  white_train[,i] <- knn(wtrain, wtrain, cl = wtrain_label, k = i)
}

white_eval.train<-as.data.frame(matrix(0,ncol = 1, nrow = 80))
names(white_eval.train)[1]<-"correct.prediction"

for (i in seq(1:80)) {
  white_eval.train[i,] <- 100*sum(wtrain_label == white_train[,i])/nrow(wtrain)
  }

white_eval.train[,"k"] <-seq(1:80)
white_eval.train$dataset <- "training"
white_eval$dataset <- "validation"
white_eval_full <- rbind(white_eval, white_eval.train)

ggplot(white_eval_full, aes(x = k, col = dataset)) + geom_line(aes(y = 100 - correct.prediction)) + scale_x_reverse() + labs(title = "k-NN performance", x = "k", y = "Prediction Error Percentage")
```

The graph above follows the expected pattern previously described. The training data performance decreases as k decreases until it reaches perfect accuracy at k = 1. Additionally, the validation set performance follows a parabola with a minimum at k = `r min.error.k`. This makes sense because further decreasing k would imply overfitting which increases variance by trying to predict randomness. Finally, the validation prediction errors are always or nearly always above the training set errors. This also follows the expected pattern because the validation data has been unexposed to the values used to train the model.

####7. Predict the generalisation error using the test data set, as well as output the result in a confusion matrix.
```{r}
gen.error <- knn(wtrain, wtest, wtrain_label, k = min.error.k)
predicted <- gen.error
actual <- wtest_label
confusion.matrix <- table(predicted, actual)
generalisation.error <- 100 - (100 * sum(wtest_label == gen.error)/nrow(wtest))
```

```{r echo = FALSE}
confusion.matrix
```

The generalisation error is `r round(generalisation.error,3)`% when k = `r min.error.k`. This is equivalent to a prediction accuracy of `r 100 - round(generalisation.error,3)`% on the test data set. This number represents our error rate when using the best k value that we picked from training on the training set and selecting k from performance on the validation set. Now we provide the algorithm with new data (the test set) in order to see its performance on "untainted" data.


#### How do you judge whether the classifier is well-suited for the data set?

The best k (k = `r min.error.k`) from the validation set performs at a `r 100-round(generalisation.error,3)`% level on the test dataset. On the surface this appears to be a relatively good prediction level. However, when we examine the confusion matrix, problems emerge. The classifier is not very well suited for the dataset. It can identify good wines quite well, not often predicting them to be bad. However, the converse is not true at all. When the wine is bad, the knn algorithm essentially picks at random. It does a poor job of identifying bad wines and often misclassifies them. This is important because the majority class is good wines and wine connoisseurs will react more poorly to receiving a poor wine than their pleasure at receiving a good wine. If this algorithm is designed to filter out bad wines (for example in a restaurant setting) then it does a poor job. When a bad wine appears, the algorithm nearly equally likely to return a false positive which looks poorly on a business. 

Moreover, the knn algorithm does not perform especially well on the white wine dataset. Based on differing splits of the training and validation sets, the optimal value of k varies greatly. As seen by the graph above, the predictive power of the algorithm is barely affected by the choice of k, with only fractions of a percentage point being the difference. The optimal value appears as though it is selected from complete randomness. As seen in the graph, the validation performance does not display a parabolic shape that would suggest an optimal k, rather the function is essentially a straight line. Therefore, I would not recommend the use of the knn neighbor algorithm to be implemented in this case because of its poor performance on predicing bad wines and the large variability on an optimal k-value.