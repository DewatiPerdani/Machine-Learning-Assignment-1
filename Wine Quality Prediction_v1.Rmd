---
title: "Machine Learning - Group Project"
author: "Group 4"
date: "1/28/2017"
output: html_document
---

```{r}
library(ggplot2)
#install.packages("class")
library(class)
```

(a) Build a k-Nearest Neighbours classifier in R for “wine_quality-white.csv” that:
1. loads the data file;
2. construct a new binary column “good wine” that indicates whether the wine is good
(which we define as having a quality of 6 or higher) or not;
3. splits the data set into a training data set (~40%), a validation data set (~30%) and a test data set (~30%) — make sure you shuffle the record before the split;
4. normalises the data according to the Z-score transform;
5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;
6. evaluates each classifier on the validation set and selects the best classifier;
7. predicts the generalisation error using the test data set, as well as outputs the result in a confusion matrix.

```{r}
#1. loads the data file;
whitew<-read.csv("winequality-white.csv",sep = ";", header = TRUE)

#2. construct a new binary column “good wine” that indicates whether the wine is good (which we define as having a quality of 6 or higher) or not
whitew$goodwine <- ifelse(whitew$quality>=6, 1, 0) #binary column for wine quality

#3. splits the data set into a training data set (~40%), a validation data set (~30%) and atest data set (~30%) — make sure you shuffle the record before the split;
set.seed(2017)

# we ignore the wine quality column because it is a deterministic relationship with good_wine

#4. normalises the data according to the Z-score transform;
whitew_scale <- as.data.frame(scale(whitew[1:11]))
whitew_y <- as.data.frame(whitew[,13])
colnames(whitew_y) <- "good_wine"
whitew_scale <- cbind(whitew_scale, whitew_y)

whitew_shuffle<- whitew_scale[sample(nrow(whitew_scale)),] # shuffles observations


wtrain <- whitew_shuffle[1:1959,]
wvalid <- whitew_shuffle[1960:3429,]
wtest <- whitew_shuffle[3430:4898,] #splits data into three sets

summary(wtest)#check normalised data

#5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;

wtrain_label<-wtrain[,12]
wvalid_label<-wvalid[,12]
wtest_label<-wtest[,12]

wtrain <- wtrain[,1:11]
wvalid <- wvalid[,1:11]
wtest  <- wtest[,1:11]



white_valid <- data.frame(V1 = 1:nrow(wvalid))
white_train <- data.frame(V1 = 1:nrow(wtrain))

for (i in seq(1:80)){
  white_valid[,i] <- knn(wtrain, wvalid, cl = wtrain_label, k = i)
  white_train[,i] <- knn(wtrain, wtrain, cl = wtrain_label, k = i)
}

#6. evaluates each classifier on the validation set and selects the best classifier;
white_eval<-as.data.frame(matrix(0,ncol = 1, nrow = 80))
names(white_eval)[1]<-"correct.prediction"

white_eval.train<-as.data.frame(matrix(0,ncol = 1, nrow = 80))
names(white_eval.train)[1]<-"correct.prediction"

for (i in seq(1:80)){
  white_eval[i,] <- 100*sum(wvalid_label == white_valid[,i])/nrow(wvalid)
  white_eval.train[i,] <- 100*sum(wtrain_label == white_train[,i])/nrow(wtrain)
  }

white_eval[,"k"]<-seq(1:80)
white_eval.train[,"k"] <-seq(1:80)

white_eval$dataset <- "validation"
white_eval.train$dataset <- "training"

white_eval_full <- rbind(white_eval, white_eval.train)

ggplot(white_eval_full, aes(x = k, col = dataset)) + geom_line(aes(y = 100 - correct.prediction)) + scale_x_reverse()

max(white_eval$correct.prediction)
white_eval$k[white_eval$correct.prediction == max(white_eval$correct.prediction)]

#best classifier is k=18 with percentage of correct prediction 76.73469%

#7. predicts the generalisation error using the test data set, as well as outputs the result in a confusion matrix.
gen.error <- knn(wtrain, wtest, wtrain_label, k = white_eval$k[white_eval$correct.prediction == max(white_eval$correct.prediction)])
predicted <- gen.error
actual <- wtest_label
table(predicted, actual) #confusion matrix 
100 * sum(wtest_label == gen.error)/nrow(wtest) #proportion of correct classification for k=18 is 76.17427%

```

knn is not a suitable classifier because it barely distinguishes between the 0 class. It picks at random. Little lift. Very sensitive to k and the choice of k does not seem to matter.